<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Realtime Gesture recognition/prediction by haojian</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Realtime Gesture recognition/prediction</h1>
        <p>real-time gesture recognition and area prediction application, wrote in C++ with QT.</p>

        <p class="view"><a href="https://github.com/haojian/gesture_recognition">View the Project on GitHub <small>haojian/gesture_recognition</small></a></p>


        <ul>
          <li><a href="https://github.com/haojian/gesture_recognition/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/haojian/gesture_recognition/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/haojian/gesture_recognition">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h3>Overview.</h3>

		<p>This gesture recognition/prediction was originally designed for the complex environment in in-car multimodal system. The motivation is to decrease the complexity of gesture operation on touch screen when the car is in motion. I rewrote the recognition algorithm recently, and built a stand-alone demo to illustrate the ideas in that algorithm.</p>

		<p>Here is the preview screenshot:
		<img src="http://shift-3.com/blog/wp-content/uploads/2011/06/Mockup_overview.png" alt="overview"></p>
		<p><b>Users can gesture to where they want to go by either tracing a particular street or drawing a circle around the area they are looking for.</b>
		</p>

		<p>Different with most past work in Gesture Recognition, this toolkit focuses in predicting the user’s intention before they had finished their gesture.</p>
		<p>Similar with <a href="http://depts.washington.edu/aimgroup/proj/dollar/">$1 gesture recognition</a>, this gesture recognition also features in there is no requirement for library, toolkit, and training.</p>

		<h3>Screen shots of stand alone demo</h3>

		<p>"POLY-LINE" & "STRAIGHT LINE":
		<img src="http://shift-3.com/blog/wp-content/uploads/2011/06/polylineline.gif" alt="polyline"></p>
		<p>
		<p>"ERAZE" & "POLY-LINE":
		<img src="http://shift-3.com/blog/wp-content/uploads/2011/06/Eraze-PolyLine.gif" alt="ERAZE"></p>
		<p>
		<p>"CIRCLE":
		<img src="http://shift-3.com/blog/wp-content/uploads/2011/06/circle.gif" alt="CIRCLE"></p>
		<p>
	
		<h3>Demo application</h3>

		<p>The <a href="https://github.com/haojian/Gesture-Demo">.exe executable program</a> is available in the package, which could run in the windows environment directly.</p>
	
		<h3>Algorithm Details</h3>
		<h4>Direction Analysis
		</h4>
		<p>
			I borrowed the idea from Optical Character Recognition, which is focus on direction analysis. This tactic could be illustrated in following image:
			<img src="http://shift-3.com/blog/wp-content/uploads/2011/06/mg_algo01.png" alt="CIRCLE"></p>
		</p>
		<ul style="list-style-type:none;">
			<li>1. Each letter is defined by a n ‘ 8-directions gesture sequence.</li>
			<li>2. The mouse moves are saved with the same 8-directions sensibility.</li>
			<li>3. A Levenshtein distance is calculated from each letter to the user moves.</li>
			<li>4. The algorithm return the best candidate (lowest levenshtein cost)</li>
		</ul>
		
		<h4>Recognition Process.</h4>
		<h5>Removing Noise.</h5>
		<p>
			<img src="http://shift-3.com/blog/wp-content/uploads/2011/06/A.png" alt="a"></p>
		<p>
			This step is to remove the noise in recording and normalize the tracking resolution. This step also can make the recognition algorithm work more efficiently.
		</p>
		
		<pre><code>
void GestureAlgorithm::addPoint(int x, int y)
{
int d_x, d_y;

d_x = x-positions.back().x;
d_y = y-positions.back().y;

if( d_x*d_x + d_y*d_y >= MIN_MOVEMENT)
{
updateStatistic(x, y);
recognizeGesture();
}
}

void GestureAlgorithm::updateStatistic(int x, int y)
{
positions.push_back(Point(x, y));
point_num = positions.size();
if(point_num >1)
{
// For Point Recognization
dist_sum += positions.begin()->dist(x,y);
dist_average =dist_sum/(point_num - 1);

// For Line Recognization
// Need a patch for the V0 calculation.
Point v0 = Point(positions[1].x - positions[0].x, positions[0].y );
Point v1 = Point(x - positions[0].x, y -positions[0].y);
if(normalize(v0) && normalize( v1))
{
float theta = acos(dot(v0, v1));
theta_sum += theta;
theta_sqsum += sq(theta);
theta_average = theta_sum / (float)(point_num - 1);
theta_factor = sqrt((float)(point_num - 1)*theta_sqsum - sq(theta_sum))/(point_num-1);
}
}
mainDirections = detectDirection(positions);

//Statistic Update
pos_x_sum += x;
pos_y_sum += y;
pos_xx_sum += sq(x);
pos_xy_sum += x * y;

midPoint = Point(pos_x_sum/point_num, pos_y_sum/point_num);
curGestureRender->render_bbox->addPoint(x, y);
}
		</code></pre>
		
		<h5>Converting to Directions.</h5>
		<p><img src="http://shift-3.com/blog/wp-content/uploads/2011/06/A.png" alt="CIRCLE"></p>
		<p>
			To simplify this problem, the point list is convert to a vector list (velocity). The current design simplify the vectors into 4 dimensions: up, down, left, right. But it could be easily expand to the 8 dimensions as the one stated in the OCT tactic.</p>
		<pre><code>
PosList GestureAlgorithm::limitDirections(const PosList &positions)
{
PosList res;
int lastx, lasty;
bool firstTime = true;

for( PosList::const_iterator ii = positions.begin(); ii != positions.end(); ++ii )
{
if( firstTime )
{
lastx = ii->x;
lasty = ii->y;

firstTime = false;
}
else
{
int dx, dy;

dx = ii->x – lastx;
dy = ii->y – lasty;

if( dy > 0 )
{
if( dx > dy || -dx > dy )
dy = 0;
else
dx = 0;
}
else
{
if( dx > -dy || -dx > -dy )
dy = 0;
else
dx = 0;
}
res.push_back( Point( dx, dy ) );
lastx = ii->x;
lasty = ii->y;
}
}

return res;
}
		</code></pre>
		<h5>Simplifying.</h5>
		<p><img src="http://shift-3.com/blog/wp-content/uploads/2011/06/C.png" alt="CIRCLE"/></p>
		<p>
			Simplifying is finding the consecutive movements in the same direction and joining them. This step would convert the vector list to a simple presentation (e.g., “right, up, right, up”). Then the list can easily matched to some predefined gestures. An example result of this step is as follows:
			
		</p>
		<pre><code>
	Position Num:  141
	X=  113 Y= 0
	X= 0 Y= -15
	X=  0 Y= 179
	X= 13 Y= 0
	X=  -110 Y= 0
	X= 0 Y= 6
	X=  0 Y= -101
	X= 3 Y= 0
	Directions Number: 8
	Directions Length:540
	UP Number: 3 Down Number: 2 Left: 1 right 2
	Position Num:  142
			</code></pre>
			
		<pre><code>
PosList GestureAlgorithm::simplify(const PosList &positions)
{
PosList res;
int lastdx = 0, lastdy = 0;
bool firstTime = true;
int index=0;
for( PosList::const_iterator ii = positions.begin(); ii != positions.end(); ++ii )
{
if( firstTime )
{
lastdx = ii->x;
lastdy = ii->y;
firstTime = false;
}
else
{
bool joined = false;
if( (lastdx > 0 && ii->x > 0) || (lastdx < 0 && ii->x < 0) )
{
lastdx += ii->x;
joined = true;
}
if( (lastdy > 0 && ii->y > 0) || (lastdy < 0 && ii->y < 0) )
{
lastdy += ii->y;
joined = true;
}
if( !joined )
{
res.push_back( Point( lastdx, lastdy ) );
lastdx = ii->x;
lastdy = ii->y;
}
}
}
if( lastdx != 0 || lastdy != 0 )
{
res.push_back( Point( lastdx, lastdy ) );
}
return res;
}
		</code></pre>
			
		<h5>Ignore Short Segments and Matching.</h5>

		<p><img src="http://shift-3.com/blog/wp-content/uploads/2011/06/D.png" alt="CIRCLE"></p>
		<p>Based on the assumption that each gesture should have some noise segments, I set a empirical parameter 90% as the elite threshold. This algorithm would remove the shortest segment repeatedly until the ration reached the empirical parameter.</p>

		<pre><code>
PosList GestureAlgorithm::removeShortestNoise(const PosList &positions)
{
    PosList res;
    int shortestSoFar;
    PosList::const_iterator shortest;
    bool firstTime = true;

    for( PosList::const_iterator ii = positions.begin(); ii != positions.end(); ++ii )
    {
        if( firstTime )
        {
            shortestSoFar = ii->x*ii->x + ii->y*ii->y;
            shortest = ii;

            firstTime = false;
        }
        else
        {
            if( (ii->x*ii->x + ii->y*ii->y) < shortestSoFar )             {                 shortestSoFar = ii->x*ii->x + ii->y*ii->y;
                shortest = ii;
            }
        }
    }

    for( PosList::const_iterator ii = positions.begin(); ii != positions.end(); ++ii )
    {
        if( ii != shortest)
            res.push_back( *ii );
    }

    return res;
}

PosList GestureAlgorithm::detectDirection(const PosList &positions)
{
    PosList directions = simplify(limitDirections(positions));
    double minLength = calcLength(directions) *minMatch;

    while(directions.size() > 0 && calcLength(removeShortestNoise(directions)) > minLength)
    {
        directions = simplify(removeShortestNoise(directions));
    }

    upNum = 0; downNum = 0; leftNum = 0; rightNum =0;
    for(int i = 0; i< directions.size(); i++)     {         if(directions[i].y >= 0 && directions[i].x ==0)
            upNum++;
        else if(directions[i].y < 0 && directions[i].x ==0)             downNum++;         else if(directions[i].x >= 0 && directions[i].y ==0 )
            leftNum++;
        else if(directions[i].x < 0 && directions[i].y ==0 )
            rightNum++;
    }
    return directions;
}
		</code></pre>

      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/haojian">haojian</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>